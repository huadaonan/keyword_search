#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­æ–‡è¯é¢‘ç»Ÿè®¡å·¥å…·
åˆ†ææŒ‡å®šç›®å½•ä¸‹HTMLæ–‡ä»¶ä¸­çš„ä¸­æ–‡è¯é¢‘
"""

import os
import re
import jieba
from collections import Counter
from bs4 import BeautifulSoup
import argparse
import json

def extract_text_from_html(file_path):
    """ä»HTMLæ–‡ä»¶ä¸­æå–çº¯æ–‡æœ¬å†…å®¹"""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            soup = BeautifulSoup(f.read(), 'html.parser')
            # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
            return soup.get_text()
    except:
        return ""

def clean_and_segment_text(text):
    """æ¸…ç†æ–‡æœ¬å¹¶è¿›è¡Œä¸­æ–‡åˆ†è¯"""
    # åªä¿ç•™ä¸­æ–‡å­—ç¬¦
    chinese_text = re.sub(r'[^\u4e00-\u9fff]', ' ', text)
    # åˆ†è¯å¹¶è¿‡æ»¤å•å­—å’Œåœç”¨è¯
    words = [word.strip() for word in jieba.cut(chinese_text) 
             if len(word.strip()) > 1 and word.strip() not in STOP_WORDS]
    return words

def analyze_directory(directory, min_freq=2, top_n=100):
    """åˆ†æç›®å½•ä¸‹æ‰€æœ‰HTMLæ–‡ä»¶çš„ä¸­æ–‡è¯é¢‘"""
    word_counter = Counter()
    file_count = 0
    
    print(f"æ­£åœ¨åˆ†æç›®å½•: {directory}")
    
    # éå†ç›®å½•ä¸‹çš„æ‰€æœ‰HTMLæ–‡ä»¶
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.lower().endswith(('.html', '.htm')):
                file_path = os.path.join(root, file)
                text = extract_text_from_html(file_path)
                if text:
                    words = clean_and_segment_text(text)
                    word_counter.update(words)
                    file_count += 1
                    if file_count % 10 == 0:
                        print(f"å·²å¤„ç† {file_count} ä¸ªæ–‡ä»¶...")
    
    print(f"å…±å¤„ç† {file_count} ä¸ªHTMLæ–‡ä»¶")
    
    # è¿‡æ»¤ä½é¢‘è¯å¹¶è·å–Top N
    filtered_words = {word: count for word, count in word_counter.items() 
                     if count >= min_freq}
    top_words = Counter(filtered_words).most_common(top_n)
    
    return top_words, len(word_counter), file_count

def save_results(results, total_words, file_count, output_file):
    """ä¿å­˜åˆ†æç»“æœ"""
    data = {
        "ç»Ÿè®¡ä¿¡æ¯": {
            "å¤„ç†æ–‡ä»¶æ•°": file_count,
            "æ€»è¯æ±‡æ•°": total_words,
            "é«˜é¢‘è¯æ•°": len(results)
        },
        "è¯é¢‘ç»Ÿè®¡": [{"è¯æ±‡": word, "é¢‘æ¬¡": count} for word, count in results]
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def print_results(results, total_words, file_count):
    """æ‰“å°åˆ†æç»“æœ"""
    print(f"\n{'='*50}")
    print(f"ğŸ“Š ä¸­æ–‡è¯é¢‘ç»Ÿè®¡æŠ¥å‘Š")
    print(f"{'='*50}")
    print(f"å¤„ç†æ–‡ä»¶æ•°: {file_count}")
    print(f"æ€»è¯æ±‡æ•°: {total_words}")
    print(f"é«˜é¢‘è¯æ•°: {len(results)}")
    print(f"{'='*50}")
    
    print(f"{'æ’å':<4} {'è¯æ±‡':<20} {'é¢‘æ¬¡':<8} {'å æ¯”'}")
    print("-" * 50)
    
    total_freq = sum(count for _, count in results)
    for i, (word, count) in enumerate(results[:20], 1):
        percentage = (count / total_freq) * 100
        print(f"{i:<4} {word:<20} {count:<8} {percentage:.2f}%")

# å¸¸ç”¨åœç”¨è¯åˆ—è¡¨
STOP_WORDS = {
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 
    'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
    'è‡ªå·±', 'è¿™', 'é‚£', 'é‡Œ', 'å', 'ä»€ä¹ˆ', 'è¿™ä¸ª', 'æ€ä¹ˆ', 'å¯ä»¥', 'æ²¡', 'èƒ½',
    'è€Œä¸”', 'ä½†æ˜¯', 'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'è™½ç„¶', 'ç„¶å', 'æˆ–è€…', 'æ¯”è¾ƒ', 'éå¸¸',
    'å·²ç»', 'è¿˜æ˜¯', 'åº”è¯¥', 'å¯èƒ½', 'éœ€è¦', 'é€šè¿‡', 'è¿›è¡Œ', 'å®ç°', 'æä¾›', 'ä½¿ç”¨',
    'åŒ…æ‹¬', 'ä¸»è¦', 'åŸºæœ¬', 'ä¸€èˆ¬', 'ç‰¹åˆ«', 'å°¤å…¶', 'ä¾‹å¦‚', 'æ¯”å¦‚', 'è¿™æ ·', 'é‚£æ ·'
}

def main():
    parser = argparse.ArgumentParser(description='ä¸­æ–‡è¯é¢‘ç»Ÿè®¡å·¥å…·')
    parser.add_argument('directory', nargs='?', default='/opt/homebrew/var/www',
                       help='è¦åˆ†æçš„ç›®å½•è·¯å¾„')
    parser.add_argument('-m', '--min-freq', type=int, default=2,
                       help='æœ€å°è¯é¢‘é˜ˆå€¼ (é»˜è®¤: 2)')
    parser.add_argument('-n', '--top-n', type=int, default=100,
                       help='æ˜¾ç¤ºå‰Nä¸ªé«˜é¢‘è¯ (é»˜è®¤: 100)')
    parser.add_argument('-o', '--output', default='word_frequency.json',
                       help='è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: word_frequency.json)')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.directory):
        print(f"é”™è¯¯: ç›®å½• '{args.directory}' ä¸å­˜åœ¨")
        return
    
    # æ‰§è¡Œè¯é¢‘åˆ†æ
    results, total_words, file_count = analyze_directory(
        args.directory, args.min_freq, args.top_n)
    
    if not results:
        print("æœªæ‰¾åˆ°ä»»ä½•ä¸­æ–‡å†…å®¹æˆ–HTMLæ–‡ä»¶")
        return
    
    # æ˜¾ç¤ºç»“æœ
    print_results(results, total_words, file_count)
    
    # ä¿å­˜ç»“æœ
    save_results(results, total_words, file_count, args.output)

if __name__ == "__main__":
    main()
